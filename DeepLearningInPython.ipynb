{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def softmax(x):\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    if(len(x.shape) > 1): # matrix\n",
    "        rowmax = x.max(axis = 1, keepdims = True)\n",
    "        x = x - rowmax\n",
    "        exp = np.exp(x)\n",
    "        rowsum = exp.sum(axis = 1,keepdims = True)\n",
    "        return exp / rowsum\n",
    "        \n",
    "        \n",
    "    else:  # vector\n",
    "        maxval = x.max()\n",
    "        x = x - maxval\n",
    "        exp = np.exp(x)\n",
    "        sums = exp.sum()\n",
    "        return exp / sums\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    \"\"\"\n",
    "    deno = 1 + np.exp(-x)\n",
    "   \n",
    "    return 1. / deno\n",
    "\n",
    "def sigmoid_grad(f):\n",
    "    \"\"\"\n",
    "    Compute the gradient for the sigmoid function here. Note that\n",
    "    for this implementation, the input f should be the sigmoid\n",
    "    function value of your original input x. \n",
    "    \"\"\"\n",
    "\n",
    "    right = 1 - f\n",
    "    return np.multiply(f, right)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    \"\"\" \n",
    "    Forward and backward propagation for a two-layer  network hidden is sigmoidal and outer is softmax\n",
    "    \n",
    "    Compute the forward propagation and for the cross entropy cost,\n",
    "    and backward propagation for the gradients for all parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    \n",
    "    hidden = sigmoid(data.dot(W1) + b1)\n",
    "    prediction = softmax(hidden.dot(W2) + b2)\n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), \n",
    "        gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        ### try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        ### YOUR CODE HERE:\n",
    "        x[ix] += h\n",
    "        random.setstate(rndstate)  \n",
    "        (costPos, d) = f(x)\n",
    "        x[ix] -= 2*h\n",
    "        random.setstate(rndstate)\n",
    "        (costNeg, d) = f(x)\n",
    "        numgrad = (costPos - costNeg) / (2. * h)\n",
    "        x[ix] += h\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad)\n",
    "            return\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
